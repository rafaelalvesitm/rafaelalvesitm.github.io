---
---

A entropia pode ser pensada como uma medida da informação transmitida ou a mediada de quanta ifnroamccão recebemos de uma determinada distribuição de probabilidade. Quanto maior a variação dos dados maior é a entropia dos mesmo. É dada pela equação:

$$H(p) = -\sum_i p_i \log_2(p_i)$$

Para um exemplo simples imagine que uma estação meteorológica possa dizer se amanhã vai chover ou fazer sol. E que a probabilidade de chuvor em dado dia é de 25% e de fazer sol é de 75%. Logo tem-se: $H(p) = -0.25 \log_2(0.25) - 0.75 \log_2(0.75) = 0.811278$. 